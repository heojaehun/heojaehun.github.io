+++
title = "Ollama + vscode"
date = "2025-10-27T21:36:59+09:00"

tags = ["도구"]
+++

## Ollama 설치

1. 다음 주소에서 환경에 맞는 ollama를 다운 받는다. `https://ollama.com/download`
2. 설치를 진행한다.
3. 끝

## 모델 다운로드
### GUI로 다운로드 하기
1. Ollama를 실행한다. 
2. 채팅 창 오른쪽 하단에 있는 선택 상자에서 원하는 모델을 선택한다.
3. 채팅창에 아무 질문이나 입력하고 엔터 키를 누른다. 그러면 별다른 설정 없이 모델이 다운로드 된다.
4. 다운로드를 완료하면 처음에 입력했던 질문에 답을 해줄 것이다.
5. 끝

### CLI로 다운로드 하기
1. 터미널을 연다. 
2. `ollama pull {모델명}`이라고 입력하면 모델 다운로드를 시작한다.
	- Ollama에서 사용할 수 있는 모델은 `https://ollama.com/search`에서 확인할 수 있다. 
	- GUI에는 사용할 수 있는 모든 모델을 찾을 수 없다. 
3. 끝

## Ollama 사용하기
### GUI로 사용하기
1. 모델을 다운로드하고 나면 곧바로 채팅을 이어나갈 수 있다. 매우 간편.

### CLI로 사용하기
1. 터미널에서 `ollama run {모델명}`명령으로 실행할 수 있다. 역시 매우 간편.
	-  CLI에서는 모델이 출력하는 마크다운 형식의 답이 그대로 노출되므로 보기에 좋지 않을 수 있다. 
	-  채팅 형식으로 사용을 한다면 GUI를 사용하는 것이 사용 경험이 좋다.
	-  나는 CLI를 모델 다운로드 할 때만 사용했다.

## VSCode와 연동하기
1. VSCode에서 **Continue**라는 확장을 설치한다.
2. 설치가 완료되면 왼쪽 Acivity Bar에 추가된 Continue 아이콘을 클릭한다. 
3. Continue를 위한 Side bar가 열리면 채팅 창 하단의 `Select model` 버튼을 누른다.
4. Add Chat model 창이 열리면 아래와 같이 선택한다.
	- Provider: Ollama
	- Model: Autodetect
5. Connect 버튼을 클릭한다. 
6. 그러면 다시 채팅창으로 돌아가고 현재 시스템에 설치된 모델이 선택되어 있을 것이다.
7. Ollama와 VSCode의 연동 되었다. 이제 마음껏 LLM와 함께 프로젝트를 진행할 수 있다. 
